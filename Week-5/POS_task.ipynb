{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Copy of POS-task.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCqxIizrbDpN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "d5099df8-bcca-431d-8ace-23217559b774"
      },
      "source": [
        "! shred -u setup_google_colab.py\n",
        "! wget https://raw.githubusercontent.com/hse-aml/intro-to-dl/master/setup_google_colab.py -O setup_google_colab.py\n",
        "import setup_google_colab\n",
        "# please, uncomment the week you're working on\n",
        "# setup_google_colab.setup_week1()\n",
        "# setup_google_colab.setup_week2()\n",
        "# setup_google_colab.setup_week2_honor()\n",
        "# setup_google_colab.setup_week3()\n",
        "# setup_google_colab.setup_week4()\n",
        "setup_google_colab.setup_week5()\n",
        "# setup_google_colab.setup_week6()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shred: setup_google_colab.py: failed to open for writing: No such file or directory\n",
            "--2020-07-02 10:57:25--  https://raw.githubusercontent.com/hse-aml/intro-to-dl/master/setup_google_colab.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3636 (3.6K) [text/plain]\n",
            "Saving to: ‘setup_google_colab.py’\n",
            "\n",
            "setup_google_colab. 100%[===================>]   3.55K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-07-02 10:57:25 (39.9 MB/s) - ‘setup_google_colab.py’ saved [3636/3636]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrCqLN3oa8z5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2108acc9-daab-45e8-b1a2-c403243fb4f4"
      },
      "source": [
        "# set tf 1.x for colab\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdU43NYNa80C",
        "colab_type": "text"
      },
      "source": [
        "__This seminar:__ after you're done coding your own recurrent cells, it's time you learn how to train recurrent networks easily with Keras. We'll also learn some tricks on how to use keras layers and model. We also want you to note that this is a non-graded assignment, meaning you are not required to pass it for a certificate.\n",
        "\n",
        "Enough beatin' around the bush, let's get to the task!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "f7fBk3OSa80G",
        "colab_type": "text"
      },
      "source": [
        "## Part Of Speech Tagging\n",
        "\n",
        "<img src=https://i.stack.imgur.com/6pdIT.png width=320>\n",
        "\n",
        "Unlike our previous experience with language modelling, this time around we learn the mapping between two different kinds of elements.\n",
        "\n",
        "This setting is common for a range of useful problems:\n",
        "* Speech Recognition - processing human voice into text\n",
        "* Part Of Speech Tagging - for morphology-aware search and as an auxuliary task for most NLP problems\n",
        "* Named Entity Recognition - for chat bots and web crawlers\n",
        "* Protein structure prediction - for bioinformatics\n",
        "\n",
        "Our current guest is part-of-speech tagging. As the name suggests, it's all about converting a sequence of words into a sequence of part-of-speech tags. We'll use a reduced tag set for simplicity:\n",
        "\n",
        "### POS-tags\n",
        "- ADJ - adjective (new, good, high, ...)\n",
        "- ADP - adposition\t(on, of, at, ...)\n",
        "- ADV - adverb\t(really, already, still, ...)\n",
        "- CONJ\t- conjunction\t(and, or, but, ...)\n",
        "- DET - determiner, article\t(the, a, some, ...)\n",
        "- NOUN\t- noun\t(year, home, costs, ...)\n",
        "- NUM - numeral\t(twenty-four, fourth, 1991, ...)\n",
        "- PRT -\tparticle (at, on, out, ...)\n",
        "- PRON - pronoun (he, their, her, ...)\n",
        "- VERB - verb (is, say, told, ...)\n",
        "- .\t- punctuation marks\t(. , ;)\n",
        "- X\t- other\t(ersatz, esprit, dunno, ...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "h5zsDQ5Ya80I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f11ac30a-fd36-4eec-f059-57cb1bb62969"
      },
      "source": [
        "import nltk\n",
        "import sys\n",
        "import numpy as np\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')\n",
        "all_tags = ['#EOS#','#UNK#','ADV', 'NOUN', 'ADP', 'PRON', 'DET', '.', 'PRT', 'VERB', 'X', 'NUM', 'CONJ', 'ADJ']\n",
        "\n",
        "data = np.array([ [(word.lower(),tag) for word,tag in sentence] for sentence in data ])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYHNxsa7a80N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_data,test_data = train_test_split(data,test_size=0.25,random_state=42)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HQByGmBa80S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "bd4247c9-10e7-46dc-9886-595e812a3707"
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "def draw(sentence):\n",
        "    words,tags = zip(*sentence)\n",
        "    display(HTML('<table><tr>{tags}</tr>{words}<tr></table>'.format(\n",
        "                words = '<td>{}</td>'.format('</td><td>'.join(words)),\n",
        "                tags = '<td>{}</td>'.format('</td><td>'.join(tags)))))\n",
        "    \n",
        "    \n",
        "draw(data[11])\n",
        "draw(data[10])\n",
        "draw(data[7])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table><tr><td>NOUN</td><td>ADP</td><td>NOUN</td><td>NOUN</td><td>NOUN</td><td>NOUN</td><td>VERB</td><td>ADV</td><td>VERB</td><td>ADP</td><td>DET</td><td>ADJ</td><td>NOUN</td><td>.</td></tr><td>implementation</td><td>of</td><td>georgia's</td><td>automobile</td><td>title</td><td>law</td><td>was</td><td>also</td><td>recommended</td><td>by</td><td>the</td><td>outgoing</td><td>jury</td><td>.</td><tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table><tr><td>PRON</td><td>VERB</td><td>ADP</td><td>DET</td><td>NOUN</td><td>.</td><td>VERB</td><td>NOUN</td><td>PRT</td><td>VERB</td><td>.</td><td>DET</td><td>NOUN</td><td>.</td></tr><td>it</td><td>urged</td><td>that</td><td>the</td><td>city</td><td>``</td><td>take</td><td>steps</td><td>to</td><td>remedy</td><td>''</td><td>this</td><td>problem</td><td>.</td><tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table><tr><td>NOUN</td><td>VERB</td></tr><td>merger</td><td>proposed</td><tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMd-XNKMa80c",
        "colab_type": "text"
      },
      "source": [
        "### Building vocabularies\n",
        "\n",
        "Just like before, we have to build a mapping from tokens to integer ids. This time around, our model operates on a word level, processing one word per RNN step. This means we'll have to deal with far larger vocabulary.\n",
        "\n",
        "Luckily for us, we only receive those words as input i.e. we don't have to predict them. This means we can have a large vocabulary for free by using word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0Yk3m2da80d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "29fe3512-3670-47ff-a86e-00f7365c1349"
      },
      "source": [
        "from collections import Counter\n",
        "word_counts = Counter()\n",
        "for sentence in data:\n",
        "    words,tags = zip(*sentence)\n",
        "    word_counts.update(words)\n",
        "\n",
        "all_words = ['#EOS#','#UNK#']+list(list(zip(*word_counts.most_common(10000)))[0])\n",
        "\n",
        "#let's measure what fraction of data words are in the dictionary\n",
        "print(\"Coverage = %.5f\"%(float(sum(word_counts[w] for w in all_words)) / sum(word_counts.values())))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Coverage = 0.92876\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE09Wswba80h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "word_to_id = defaultdict(lambda:1,{word:i for i,word in enumerate(all_words)})\n",
        "tag_to_id = {tag:i for i,tag in enumerate(all_tags)}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIBOqN9ra80p",
        "colab_type": "text"
      },
      "source": [
        "convert words and tags into fixed-size matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3C_DUura80s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_matrix(lines,token_to_id,max_len=None,pad=0,dtype='int32',time_major=False):\n",
        "    \"\"\"Converts a list of names into rnn-digestable matrix with paddings added after the end\"\"\"\n",
        "    \n",
        "    max_len = max_len or max(map(len,lines))\n",
        "    matrix = np.empty([len(lines),max_len],dtype)\n",
        "    matrix.fill(pad)\n",
        "\n",
        "    for i in range(len(lines)):\n",
        "        line_ix = list(map(token_to_id.__getitem__,lines[i]))[:max_len]\n",
        "        matrix[i,:len(line_ix)] = line_ix\n",
        "\n",
        "    return matrix.T if time_major else matrix\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-1sGQGaa80y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "96f4f46d-d505-477e-f614-98ee7da844a1"
      },
      "source": [
        "batch_words,batch_tags = zip(*[zip(*sentence) for sentence in data[-3:]])\n",
        "\n",
        "print(\"Word ids:\")\n",
        "print(to_matrix(batch_words,word_to_id))\n",
        "print(\"Tag ids:\")\n",
        "print(to_matrix(batch_tags,tag_to_id))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word ids:\n",
            "[[   2 3057    5    2 2238 1334 4238 2454    3    6   19   26 1070   69\n",
            "     8 2088    6    3    1    3  266   65  342    2    1    3    2  315\n",
            "     1    9   87  216 3322   69 1558    4    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0]\n",
            " [  45   12    8  511 8419    6   60 3246   39    2    1    1    3    2\n",
            "   845    1    3    1    3   10 9910    2    1 3470    9   43    1    1\n",
            "     3    6    2 1046  385   73 4562    3    9    2    1    1 3250    3\n",
            "    12   10    2  861 5240   12    8 8936  121    1    4]\n",
            " [  33   64   26   12  445    7 7346    9    8 3337    3    1 2811    3\n",
            "     2  463  572    2    1    1 1649   12    1    4    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0]]\n",
            "Tag ids:\n",
            "[[ 6  3  4  6  3  3  9  9  7 12  4  5  9  4  6  3 12  7  9  7  9  8  4  6\n",
            "   3  7  6 13  3  4  6  3  9  4  3  7  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0]\n",
            " [ 5  9  6  9  3 12  6  3  7  6 13  3  7  6 13  3  7 13  7  5  9  6  3  3\n",
            "   4  6 13  3  7 12  6  3  6 13  3  7  4  6  3  9  3  7  9  4  6 13  3  9\n",
            "   6  3  2 13  7]\n",
            " [ 4  6  5  9 13  4  3  4  6 13  7 13  3  7  6  3  4  6 13  3  3  9  9  7\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "-pRziShJa80_",
        "colab_type": "text"
      },
      "source": [
        "### Build model\n",
        "\n",
        "Unlike our previous lab, this time we'll focus on a high-level keras interface to recurrent neural networks. It is as simple as you can get with RNN, allbeit somewhat constraining for complex tasks like seq2seq.\n",
        "\n",
        "By default, all keras RNNs apply to a whole sequence of inputs and produce a sequence of hidden states `(return_sequences=True` or just the last hidden state `(return_sequences=False)`. All the recurrence is happening under the hood.\n",
        "\n",
        "At the top of our model we need to apply a Dense layer to each time-step independently. As of now, by default keras.layers.Dense would apply once to all time-steps concatenated. We use __keras.layers.TimeDistributed__ to modify Dense layer so that it would apply across both batch and time axes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNc7CHh3a81A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "3fff99f0-fbd0-45f2-ac61-9bc918c08e71"
      },
      "source": [
        "import keras\n",
        "import keras.layers as L\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(L.InputLayer([None],dtype='int32'))\n",
        "model.add(L.Embedding(len(all_words),50))\n",
        "model.add(L.SimpleRNN(64,return_sequences=True))\n",
        "\n",
        "#add top layer that predicts tag probabilities\n",
        "stepwise_dense = L.Dense(len(all_tags),activation='softmax')\n",
        "stepwise_dense = L.TimeDistributed(stepwise_dense)\n",
        "model.add(stepwise_dense)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:59: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:432: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3535: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1192: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJHhsmi0a81F",
        "colab_type": "text"
      },
      "source": [
        "__Training:__ in this case we don't want to prepare the whole training dataset in advance. The main cause is that the length of every batch depends on the maximum sentence length within the batch. This leaves us two options: use custom training code as in previous seminar or use generators.\n",
        "\n",
        "Keras models have a __`model.fit_generator`__ method that accepts a python generator yielding one batch at a time. But first we need to implement such generator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPEZeo6Oa81K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "BATCH_SIZE=32\n",
        "def generate_batches(sentences,batch_size=BATCH_SIZE,max_len=None,pad=0):\n",
        "    assert isinstance(sentences,np.ndarray),\"Make sure sentences is a numpy array\"\n",
        "    \n",
        "    while True:\n",
        "        indices = np.random.permutation(np.arange(len(sentences)))\n",
        "        for start in range(0,len(indices)-1,batch_size):\n",
        "            batch_indices = indices[start:start+batch_size]\n",
        "            batch_words,batch_tags = [],[]\n",
        "            for sent in sentences[batch_indices]:\n",
        "                words,tags = zip(*sent)\n",
        "                batch_words.append(words)\n",
        "                batch_tags.append(tags)\n",
        "\n",
        "            batch_words = to_matrix(batch_words,word_to_id,max_len,pad)\n",
        "            batch_tags = to_matrix(batch_tags,tag_to_id,max_len,pad)\n",
        "            \n",
        "            batch_tags_1hot = to_categorical(batch_tags,len(all_tags)).reshape(batch_tags.shape+(-1,))\n",
        "            yield batch_words,batch_tags_1hot\n",
        "        "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uom3NKD3a81P",
        "colab_type": "text"
      },
      "source": [
        "__Callbacks:__ Another thing we need is to measure model performance. The tricky part is not to count accuracy after sentence ends (on padding) and making sure we count all the validation data exactly once.\n",
        "\n",
        "While it isn't impossible to persuade Keras to do all of that, we may as well write our own callback that does that.\n",
        "Keras callbacks allow you to write a custom code to be ran once every epoch or every minibatch. We'll define one via LambdaCallback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ri8YeyDma81R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_test_accuracy(model):\n",
        "    test_words,test_tags = zip(*[zip(*sentence) for sentence in test_data])\n",
        "    test_words,test_tags = to_matrix(test_words,word_to_id),to_matrix(test_tags,tag_to_id)\n",
        "\n",
        "    #predict tag probabilities of shape [batch,time,n_tags]\n",
        "    predicted_tag_probabilities = model.predict(test_words,verbose=1)\n",
        "    predicted_tags = predicted_tag_probabilities.argmax(axis=-1)\n",
        "\n",
        "    #compute accurary excluding padding\n",
        "    numerator = np.sum(np.logical_and((predicted_tags == test_tags),(test_words != 0)))\n",
        "    denominator = np.sum(test_words != 0)\n",
        "    return float(numerator)/denominator\n",
        "\n",
        "\n",
        "class EvaluateAccuracy(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self,epoch,logs=None):\n",
        "        sys.stdout.flush()\n",
        "        print(\"\\nMeasuring validation accuracy...\")\n",
        "        acc = compute_test_accuracy(self.model)\n",
        "        print(\"\\nValidation accuracy: %.5f\\n\"%acc)\n",
        "        sys.stdout.flush()\n",
        "        "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-Oxt7IJa81V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7366539a-5674-4501-f8cb-a3f2806e2c6f"
      },
      "source": [
        "model.compile('adam','categorical_crossentropy')\n",
        "\n",
        "model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
        "                    callbacks=[EvaluateAccuracy()], epochs=5,)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:697: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2749: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:879: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:602: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:866: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2289: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:154: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:159: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:333: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:341: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "1343/1343 [============================>.] - ETA: 0s - loss: 0.2421\n",
            "Measuring validation accuracy...\n",
            "14240/14335 [============================>.] - ETA: 0s\n",
            "Validation accuracy: 0.94053\n",
            "\n",
            "1344/1343 [==============================] - 34s - loss: 0.2420    \n",
            "Epoch 2/5\n",
            "1342/1343 [============================>.] - ETA: 0s - loss: 0.0581\n",
            "Measuring validation accuracy...\n",
            "14335/14335 [==============================] - 4s     \n",
            "\n",
            "Validation accuracy: 0.94421\n",
            "\n",
            "1344/1343 [==============================] - 36s - loss: 0.0581    \n",
            "Epoch 3/5\n",
            "1342/1343 [============================>.] - ETA: 0s - loss: 0.0515\n",
            "Measuring validation accuracy...\n",
            "14304/14335 [============================>.] - ETA: 0s\n",
            "Validation accuracy: 0.94584\n",
            "\n",
            "1344/1343 [==============================] - 36s - loss: 0.0515    \n",
            "Epoch 4/5\n",
            "1342/1343 [============================>.] - ETA: 0s - loss: 0.0473\n",
            "Measuring validation accuracy...\n",
            "14304/14335 [============================>.] - ETA: 0s\n",
            "Validation accuracy: 0.94634\n",
            "\n",
            "1344/1343 [==============================] - 37s - loss: 0.0473    \n",
            "Epoch 5/5\n",
            "1342/1343 [============================>.] - ETA: 0s - loss: 0.0431\n",
            "Measuring validation accuracy...\n",
            "14272/14335 [============================>.] - ETA: 0s\n",
            "Validation accuracy: 0.94556\n",
            "\n",
            "1344/1343 [==============================] - 37s - loss: 0.0431    \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f58e061a0f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb1OiC9_a81e",
        "colab_type": "text"
      },
      "source": [
        "Measure final accuracy on the whole test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVPLdo1la81f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11e851a1-4ff0-4197-b2ac-654a6cfba6f3"
      },
      "source": [
        "acc = compute_test_accuracy(model)\n",
        "print(\"Final accuracy: %.5f\"%acc)\n",
        "\n",
        "assert acc>0.94, \"Keras has gone on a rampage again, please contact course staff.\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14272/14335 [============================>.] - ETA: 0sFinal accuracy: 0.94556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABJP2B78a81j",
        "colab_type": "text"
      },
      "source": [
        "### Task I: getting all bidirectional\n",
        "\n",
        "Since we're analyzing a full sequence, it's legal for us to look into future data.\n",
        "\n",
        "A simple way to achieve that is to go both directions at once, making a __bidirectional RNN__.\n",
        "\n",
        "In Keras you can achieve that both manually (using two LSTMs and Concatenate) and by using __`keras.layers.Bidirectional`__. \n",
        "\n",
        "This one works just as `TimeDistributed` we saw before: you wrap it around a recurrent layer (SimpleRNN now and LSTM/GRU later) and it actually creates two layers under the hood.\n",
        "\n",
        "Your first task is to use such a layer for our POS-tagger."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsjpCBksa81k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define a model that utilizes bidirectional SimpleRNN\n",
        "model = keras.models.Sequential()\n",
        "model.add(L.InputLayer([None],dtype='int32'))\n",
        "model.add(L.Embedding(len(all_words),50))\n",
        "simple_rnn=L.SimpleRNN(64,return_sequences=True)\n",
        "simple_rnn=L.Bidirectional(simple_rnn)\n",
        "model.add(simple_rnn)\n",
        "\n",
        "\n",
        "#add top layer that predicts tag probabilities\n",
        "stepwise_dense = L.Dense(len(all_tags),activation='softmax')\n",
        "stepwise_dense = L.TimeDistributed(stepwise_dense)\n",
        "model.add(stepwise_dense)\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbxRn-hpa81u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.compile('adam','categorical_crossentropy')\n",
        "\n",
        "# model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
        "#                     callbacks=[EvaluateAccuracy()], epochs=5,)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Rkz28POa81x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1fdf8cbc-9b98-4f82-a567-d1838da905bd"
      },
      "source": [
        "acc = compute_test_accuracy(model)\n",
        "print(\"\\nFinal accuracy: %.5f\"%acc)\n",
        "\n",
        "assert acc>0.96, \"Bidirectional RNNs are better than this!\"\n",
        "print(\"Well done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14335/14335 [==============================] - 8s     \n",
            "\n",
            "Final accuracy: 0.96156\n",
            "Well done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHcjTBFWa812",
        "colab_type": "text"
      },
      "source": [
        "### Task II: now go and improve it\n",
        "\n",
        "You guesses it. We're now gonna ask you to come up with a better network.\n",
        "\n",
        "Here's a few tips:\n",
        "\n",
        "* __Go beyond SimpleRNN__: there's `keras.layers.LSTM` and `keras.layers.GRU`\n",
        "  * If you want to use a custom recurrent Cell, read [this](https://keras.io/layers/recurrent/#rnn)\n",
        "  * You can also use 1D Convolutions (`keras.layers.Conv1D`). They are often as good as recurrent layers but with less overfitting.\n",
        "* __Stack more layers__: if there is a common motif to this course it's about stacking layers\n",
        "  * You can just add recurrent and 1dconv layers on top of one another and keras will understand it\n",
        "  * Just remember that bigger networks may need more epochs to train\n",
        "* __Gradient clipping__: If your training isn't as stable as you'd like, set `clipnorm` in your optimizer.\n",
        "  * Which is to say, it's a good idea to watch over your loss curve at each minibatch. Try tensorboard callback or something similar.\n",
        "* __Regularization__: you can apply dropouts as usuall but also in an RNN-specific way\n",
        "  * `keras.layers.Dropout` works inbetween RNN layers\n",
        "  * Recurrent layers also have `recurrent_dropout` parameter\n",
        "* __More words!__: You can obtain greater performance by expanding your model's input dictionary from 5000 to up to every single word!\n",
        "  * Just make sure your model doesn't overfit due to so many parameters.\n",
        "  * Combined with regularizers or pre-trained word-vectors this could be really good cuz right now our model is blind to >5% of words.\n",
        "* __The most important advice__: don't cram in everything at once!\n",
        "  * If you stuff in a lot of modiffications, some of them almost inevitably gonna be detrimental and you'll never know which of them are.\n",
        "  * Try to instead go in small iterations and record experiment results to guide further search.\n",
        "  \n",
        "There's some advanced stuff waiting at the end of the notebook.\n",
        "  \n",
        "Good hunting!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOHb-Llba813",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define a model that utilizes bidirectional SimpleRNN\n",
        "from keras import regularizers\n",
        "model = keras.models.Sequential()\n",
        "model.add(L.InputLayer([None],dtype='int32'))\n",
        "model.add(L.Embedding(len(all_words),50))\n",
        "LSTM_layer_1=L.LSTM(64,return_sequences=True,kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))\n",
        "LSTM_layer_1=L.Bidirectional(LSTM_layer_1)\n",
        "model.add(LSTM_layer_1)\n",
        "LSTM_layer_2=L.LSTM(128,return_sequences=True,kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))\n",
        "LSTM_layer_2=L.Bidirectional(LSTM_layer_2)\n",
        "model.add(LSTM_layer_2)\n",
        "LSTM_layer_3=L.LSTM(256,return_sequences=True,kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))\n",
        "LSTM_layer_3=L.Bidirectional(LSTM_layer_3)\n",
        "model.add(LSTM_layer_3)\n",
        "\n",
        "\n",
        "\n",
        "#add top layer that predicts tag probabilities\n",
        "\n",
        "fc_1=L.Dense(1024,activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))\n",
        "fc_1=L.TimeDistributed(fc_1)\n",
        "stepwise_dense = L.Dense(len(all_tags),activation='softmax')\n",
        "stepwise_dense = L.TimeDistributed(stepwise_dense)\n",
        "model.add(stepwise_dense)\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4GDAQlda82A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "251a71d5-303a-4109-e7c0-a066e8498f01"
      },
      "source": [
        "#feel free to change anything here\n",
        "\n",
        "model.compile('adam','categorical_crossentropy')\n",
        "print(model.summary())\n",
        "model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
        "                    callbacks=[EvaluateAccuracy()], epochs=15,)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, None)              0         \n",
            "_________________________________________________________________\n",
            "embedding_4 (Embedding)      (None, None, 50)          500100    \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, None, 128)         58880     \n",
            "_________________________________________________________________\n",
            "bidirectional_6 (Bidirection (None, None, 256)         263168    \n",
            "_________________________________________________________________\n",
            "bidirectional_7 (Bidirection (None, None, 512)         1050624   \n",
            "_________________________________________________________________\n",
            "time_distributed_5 (TimeDist (None, None, 14)          7182      \n",
            "=================================================================\n",
            "Total params: 1,879,954\n",
            "Trainable params: 1,879,954\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/15\n",
            "1343/1343 [============================>.] - ETA: 0s - loss: 0.2848\n",
            "Measuring validation accuracy...\n",
            "14335/14335 [==============================] - 227s   \n",
            "\n",
            "Validation accuracy: 0.95239\n",
            "\n",
            "1344/1343 [==============================] - 1316s - loss: 0.2846  \n",
            "Epoch 2/15\n",
            "1343/1343 [============================>.] - ETA: 0s - loss: 0.0595\n",
            "Measuring validation accuracy...\n",
            "14335/14335 [==============================] - 224s   \n",
            "\n",
            "Validation accuracy: 0.95722\n",
            "\n",
            "1344/1343 [==============================] - 1301s - loss: 0.0595  \n",
            "Epoch 3/15\n",
            "1343/1343 [============================>.] - ETA: 0s - loss: 0.0687\n",
            "Measuring validation accuracy...\n",
            "14335/14335 [==============================] - 221s   \n",
            "\n",
            "Validation accuracy: 0.95781\n",
            "\n",
            "1344/1343 [==============================] - 1308s - loss: 0.0687  \n",
            "Epoch 4/15\n",
            "1343/1343 [============================>.] - ETA: 0s - loss: 0.0470\n",
            "Measuring validation accuracy...\n",
            "14335/14335 [==============================] - 223s   \n",
            "\n",
            "Validation accuracy: 0.96078\n",
            "\n",
            "1344/1343 [==============================] - 1306s - loss: 0.0470  \n",
            "Epoch 5/15\n",
            "1343/1343 [============================>.] - ETA: 0s - loss: 0.0406\n",
            "Measuring validation accuracy...\n",
            "14335/14335 [==============================] - 224s   \n",
            "\n",
            "Validation accuracy: 0.96147\n",
            "\n",
            "1344/1343 [==============================] - 1312s - loss: 0.0406  \n",
            "Epoch 6/15\n",
            "1343/1343 [============================>.] - ETA: 0s - loss: 0.0371\n",
            "Measuring validation accuracy...\n",
            "14335/14335 [==============================] - 225s   \n",
            "\n",
            "Validation accuracy: 0.96118\n",
            "\n",
            "1344/1343 [==============================] - 1315s - loss: 0.0371  \n",
            "Epoch 7/15\n",
            "1343/1343 [============================>.] - ETA: 0s - loss: 0.0346\n",
            "Measuring validation accuracy...\n",
            "14335/14335 [==============================] - 223s   \n",
            "\n",
            "Validation accuracy: 0.96185\n",
            "\n",
            "1344/1343 [==============================] - 1305s - loss: 0.0346  \n",
            "Epoch 8/15\n",
            "1343/1343 [============================>.] - ETA: 0s - loss: 0.0319\n",
            "Measuring validation accuracy...\n",
            "14335/14335 [==============================] - 227s   \n",
            "\n",
            "Validation accuracy: 0.96233\n",
            "\n",
            "1344/1343 [==============================] - 1311s - loss: 0.0319  \n",
            "Epoch 9/15\n",
            "1343/1343 [============================>.] - ETA: 0s - loss: 0.0292\n",
            "Measuring validation accuracy...\n",
            "14335/14335 [==============================] - 224s   \n",
            "\n",
            "Validation accuracy: 0.96326\n",
            "\n",
            "1344/1343 [==============================] - 1310s - loss: 0.0292  \n",
            "Epoch 10/15\n",
            "1343/1343 [============================>.] - ETA: 0s - loss: 0.0271\n",
            "Measuring validation accuracy...\n",
            "14335/14335 [==============================] - 222s   \n",
            "\n",
            "Validation accuracy: 0.96204\n",
            "\n",
            "1344/1343 [==============================] - 1309s - loss: 0.0271  \n",
            "Epoch 11/15\n",
            "1343/1343 [============================>.] - ETA: 0s - loss: 0.0252\n",
            "Measuring validation accuracy...\n",
            "14335/14335 [==============================] - 228s   \n",
            "\n",
            "Validation accuracy: 0.96199\n",
            "\n",
            "1344/1343 [==============================] - 1314s - loss: 0.0252  \n",
            "Epoch 12/15\n",
            "1343/1343 [============================>.] - ETA: 0s - loss: 0.0231\n",
            "Measuring validation accuracy...\n",
            "14335/14335 [==============================] - 224s   \n",
            "\n",
            "Validation accuracy: 0.96143\n",
            "\n",
            "1344/1343 [==============================] - 1307s - loss: 0.0231  \n",
            "Epoch 13/15\n",
            "1343/1343 [============================>.] - ETA: 0s - loss: 0.0209\n",
            "Measuring validation accuracy...\n",
            "14335/14335 [==============================] - 225s   \n",
            "\n",
            "Validation accuracy: 0.95994\n",
            "\n",
            "1344/1343 [==============================] - 1320s - loss: 0.0209  \n",
            "Epoch 14/15\n",
            "1343/1343 [============================>.] - ETA: 0s - loss: 0.0196\n",
            "Measuring validation accuracy...\n",
            "14335/14335 [==============================] - 224s   \n",
            "\n",
            "Validation accuracy: 0.96006\n",
            "\n",
            "1344/1343 [==============================] - 1311s - loss: 0.0196  \n",
            "Epoch 15/15\n",
            "1343/1343 [============================>.] - ETA: 0s - loss: 0.0177\n",
            "Measuring validation accuracy...\n",
            "14335/14335 [==============================] - 225s   \n",
            "\n",
            "Validation accuracy: 0.95972\n",
            "\n",
            "1344/1343 [==============================] - 1323s - loss: 0.0177  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f32fb0620b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eUfUUeAa82E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "cd1f2ab7-ee72-463b-9d97-870039d0cdee"
      },
      "source": [
        "acc = compute_test_accuracy(model)\n",
        "print(\"\\nFinal accuracy: %.5f\"%acc)\n",
        "\n",
        "if acc >= 0.99:\n",
        "    print(\"Awesome! Sky was the limit and yet you scored even higher!\")\n",
        "elif acc >= 0.98:\n",
        "    print(\"Excellent! Whatever dark magic you used, it certainly did it's trick.\")\n",
        "elif acc >= 0.97:\n",
        "    print(\"Well done! If this was a graded assignment, you would have gotten a 100% score.\")\n",
        "elif acc > 0.96:\n",
        "    print(\"Just a few more iterations!\")\n",
        "else:\n",
        "    print(\"There seems to be something broken in the model. Unless you know what you're doing, try taking bidirectional RNN and adding one enhancement at a time to see where's the problem.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14335/14335 [==============================] - 28s    \n",
            "\n",
            "Final accuracy: 0.96141\n",
            "Just a few more iterations!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-SbYW_Ea82H",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "#### Some advanced stuff\n",
        "Here there are a few more tips on how to improve training that are a bit trickier to impliment. We strongly suggest that you try them _after_ you've got a good initial model.\n",
        "* __Use pre-trained embeddings__: you can use pre-trained weights from [there](http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/) to kickstart your Embedding layer.\n",
        "  * Embedding layer has a matrix W (layer.W) which contains word embeddings for each word in the dictionary. You can just overwrite them with tf.assign.\n",
        "  * When using pre-trained embeddings, pay attention to the fact that model's dictionary is different from your own.\n",
        "  * You may want to switch trainable=False for embedding layer in first few epochs as in regular fine-tuning.  \n",
        "* __More efficient batching__: right now TF spends a lot of time iterating over \"0\"s\n",
        "  * This happens because batch is always padded to the length of a longest sentence\n",
        "  * You can speed things up by pre-generating batches of similar lengths and feeding it with randomly chosen pre-generated batch.\n",
        "  * This technically breaks the i.i.d. assumption, but it works unless you come up with some insane rnn architectures.\n",
        "* __Structured loss functions__: since we're tagging the whole sequence at once, we might as well train our network to do so.\n",
        "  * There's more than one way to do so, but we'd recommend starting with [Conditional Random Fields](http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/)\n",
        "  * You could plug CRF as a loss function and still train by backprop. There's even some neat tensorflow [implementation](https://www.tensorflow.org/api_guides/python/contrib.crf) for you.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otbzsIqYyaVO",
        "colab_type": "text"
      },
      "source": [
        "## Conditional Random Field Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkOpri9bmpyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install tensorflow==2.1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI4a3h8qAu2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_words,train_tags = zip(*[zip(*sentence) for sentence in train_data])\n",
        "test_words,test_tags = zip(*[zip(*sentence) for sentence in test_data])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSwJ0ktBB4Cf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_tokenize(line):\n",
        "  token_line=[]\n",
        "  for word in line:\n",
        "    token_line.append(word_to_id[word])\n",
        "  return token_line\n",
        "\n",
        "def tag_tokenize(line):\n",
        "  token_line=[]\n",
        "  for word in line:\n",
        "    token_line.append(tag_to_id[word])\n",
        "  return token_line\n",
        "\n",
        "train_words_tokens=list(map(word_tokenize,train_words))\n",
        "train_tag_tokens=list(map(tag_tokenize,train_tags))\n",
        "\n",
        "test_words_tokens=list(map(word_tokenize,test_words))\n",
        "test_tag_tokens=list(map(tag_tokenize,test_tags))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-m28mziD8lT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "\n",
        "maxlen = max([len(s) for s in train_words_tokens])\n",
        "X_train=pad_sequences(maxlen=maxlen, sequences=train_words_tokens, padding=\"post\")\n",
        "sequence_lengths_input=[len(s) for s in train_words_tokens]\n",
        "\n",
        "y_train=pad_sequences(maxlen=maxlen, sequences=train_tag_tokens, padding=\"post\")\n",
        "\n",
        "# y_train=[to_categorical(i,num_classes=len(all_tags)) for i in y_train]\n",
        "\n",
        "X_test=pad_sequences(maxlen=maxlen, sequences=test_words_tokens, padding=\"post\")\n",
        "y_test=pad_sequences(maxlen=maxlen, sequences=test_tag_tokens, padding=\"post\")\n",
        "\n",
        "# y_train=[to_categorical(i,num_classes=len(all_tags)) for i in y_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43-IYBslO07m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = (\n",
        "        tf.data.Dataset\n",
        "        .from_tensor_slices((X_train, y_train,sequence_lengths_input))\n",
        "        .repeat()\n",
        "        .shuffle(2048)\n",
        "        .batch(32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e-1QF_Jyc9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers as L\n",
        "from tensorflow import keras\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(L.InputLayer(input_shape=(X_train.shape[1],),dtype='int32'))\n",
        "model.add(L.Embedding(len(all_words),50))\n",
        "LSTM_layer_1=L.LSTM(64,return_sequences=True)\n",
        "LSTM_layer_1=L.Bidirectional(LSTM_layer_1)\n",
        "model.add(LSTM_layer_1)\n",
        "model.add(L.Conv1D(64,kernel_size=3,padding='same',activation='relu'))\n",
        "\n",
        "LSTM_layer_2=L.LSTM(64,return_sequences=True)\n",
        "LSTM_layer_2=L.Bidirectional(LSTM_layer_2)\n",
        "model.add(L.Conv1D(128,kernel_size=3,padding='same',activation='relu'))\n",
        "#add top layer that predicts tag probabilities\n",
        "stepwise_dense = L.Dense(len(all_tags),activation='linear')\n",
        "stepwise_dense = L.TimeDistributed(stepwise_dense)\n",
        "model.add(stepwise_dense)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRrlZf54TuO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.config.experimental_run_functions_eagerly(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dfBXS1mMUwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "optimizer = tf.optimizers.Adam()\n",
        "@tf.function\n",
        "def train_step(inp, labels,sequence_lengths):\n",
        "  with tf.GradientTape() as tape:\n",
        "    \n",
        "    scores=model(inp)\n",
        "    log, _ =tfa.text.crf_log_likelihood(scores, labels, sequence_lengths)\n",
        "    loss = tf.reduce_mean(-log)\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv5KIT5O1foa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "366cd7e0-5ce6-4602-d86d-8db39e3abfbe"
      },
      "source": [
        "# Training step\n",
        "EPOCHS = 10\n",
        "import time\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  # initializing the hidden state at the start of every epoch\n",
        "  # initally hidden is None\n",
        "  hidden = model.reset_states()\n",
        "\n",
        "  for (batch_n, (inp, target,seq_len)) in enumerate(train):\n",
        "    loss = train_step(inp, target,seq_len)\n",
        "\n",
        "    if batch_n % 100 == 0:\n",
        "      template = 'Epoch {} Batch {} Loss {}'\n",
        "      print(template.format(epoch+1, batch_n, loss))\n",
        "\n",
        "  # saving (checkpoint) the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:644: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.identity instead.\n",
            "Epoch 1 Batch 0 Loss 52.140098571777344\n",
            "Epoch 1 Batch 100 Loss 26.716312408447266\n",
            "Epoch 1 Batch 200 Loss 7.875341892242432\n",
            "Epoch 1 Batch 300 Loss 7.8844709396362305\n",
            "Epoch 1 Batch 400 Loss 5.103466033935547\n",
            "Epoch 1 Batch 500 Loss 5.592501640319824\n",
            "Epoch 1 Batch 600 Loss 2.880920171737671\n",
            "Epoch 1 Batch 700 Loss 2.4553985595703125\n",
            "Epoch 1 Batch 800 Loss 3.395689010620117\n",
            "Epoch 1 Batch 900 Loss 4.281782150268555\n",
            "Epoch 1 Batch 1000 Loss 2.6868486404418945\n",
            "Epoch 1 Batch 1100 Loss 1.672926902770996\n",
            "Epoch 1 Batch 1200 Loss 3.157305955886841\n",
            "Epoch 1 Batch 1300 Loss 3.0042190551757812\n",
            "Epoch 1 Batch 1400 Loss 2.3309731483459473\n",
            "Epoch 1 Batch 1500 Loss 2.0890374183654785\n",
            "Epoch 1 Batch 1600 Loss 2.4760260581970215\n",
            "Epoch 1 Batch 1700 Loss 2.2927632331848145\n",
            "Epoch 1 Batch 1800 Loss 3.249093770980835\n",
            "Epoch 1 Batch 1900 Loss 3.0013015270233154\n",
            "Epoch 1 Batch 2000 Loss 2.987731695175171\n",
            "Epoch 1 Batch 2100 Loss 3.5726118087768555\n",
            "Epoch 1 Batch 2200 Loss 1.7072378396987915\n",
            "Epoch 1 Batch 2300 Loss 2.3804147243499756\n",
            "Epoch 1 Batch 2400 Loss 1.8302017450332642\n",
            "Epoch 1 Batch 2500 Loss 1.5685606002807617\n",
            "Epoch 1 Batch 2600 Loss 3.1481850147247314\n",
            "Epoch 1 Batch 2700 Loss 1.9483096599578857\n",
            "Epoch 1 Batch 2800 Loss 1.59221613407135\n",
            "Epoch 1 Batch 2900 Loss 1.9102447032928467\n",
            "Epoch 1 Batch 3000 Loss 2.272453546524048\n",
            "Epoch 1 Batch 3100 Loss 2.555264949798584\n",
            "Epoch 1 Batch 3200 Loss 1.9381412267684937\n",
            "Epoch 1 Batch 3300 Loss 2.5867576599121094\n",
            "Epoch 1 Batch 3400 Loss 1.53476881980896\n",
            "Epoch 1 Batch 3500 Loss 2.2273707389831543\n",
            "Epoch 1 Batch 3600 Loss 1.2276400327682495\n",
            "Epoch 1 Batch 3700 Loss 1.787928581237793\n",
            "Epoch 1 Batch 3800 Loss 2.4914705753326416\n",
            "Epoch 1 Batch 3900 Loss 1.7871747016906738\n",
            "Epoch 1 Batch 4000 Loss 2.1067590713500977\n",
            "Epoch 1 Batch 4100 Loss 2.428544044494629\n",
            "Epoch 1 Batch 4200 Loss 2.120373487472534\n",
            "Epoch 1 Batch 4300 Loss 1.8528995513916016\n",
            "Epoch 1 Batch 4400 Loss 1.57181978225708\n",
            "Epoch 1 Batch 4500 Loss 1.5849823951721191\n",
            "Epoch 1 Batch 4600 Loss 1.0875049829483032\n",
            "Epoch 1 Batch 4700 Loss 1.022634506225586\n",
            "Epoch 1 Batch 4800 Loss 1.4580961465835571\n",
            "Epoch 1 Batch 4900 Loss 2.3824656009674072\n",
            "Epoch 1 Batch 5000 Loss 1.9525829553604126\n",
            "Epoch 1 Batch 5100 Loss 2.0706472396850586\n",
            "Epoch 1 Batch 5200 Loss 1.5417875051498413\n",
            "Epoch 1 Batch 5300 Loss 1.756446361541748\n",
            "Epoch 1 Batch 5400 Loss 1.7350081205368042\n",
            "Epoch 1 Batch 5500 Loss 1.4061424732208252\n",
            "Epoch 1 Batch 5600 Loss 2.430748462677002\n",
            "Epoch 1 Batch 5700 Loss 2.1934804916381836\n",
            "Epoch 1 Batch 5800 Loss 1.5942049026489258\n",
            "Epoch 1 Batch 5900 Loss 1.1687145233154297\n",
            "Epoch 1 Batch 6000 Loss 1.7553520202636719\n",
            "Epoch 1 Batch 6100 Loss 1.5195544958114624\n",
            "Epoch 1 Batch 6200 Loss 1.3468129634857178\n",
            "Epoch 1 Batch 6300 Loss 1.9675285816192627\n",
            "Epoch 1 Batch 6400 Loss 1.8529130220413208\n",
            "Epoch 1 Batch 6500 Loss 1.609963297843933\n",
            "Epoch 1 Batch 6600 Loss 1.4746551513671875\n",
            "Epoch 1 Batch 6700 Loss 1.7849366664886475\n",
            "Epoch 1 Batch 6800 Loss 1.8440849781036377\n",
            "Epoch 1 Batch 6900 Loss 2.1528868675231934\n",
            "Epoch 1 Batch 7000 Loss 0.9372241497039795\n",
            "Epoch 1 Batch 7100 Loss 0.5677700638771057\n",
            "Epoch 1 Batch 7200 Loss 2.071685791015625\n",
            "Epoch 1 Batch 7300 Loss 0.5525596141815186\n",
            "Epoch 1 Batch 7400 Loss 1.2446376085281372\n",
            "Epoch 1 Batch 7500 Loss 2.155103921890259\n",
            "Epoch 1 Batch 7600 Loss 0.8496638536453247\n",
            "Epoch 1 Batch 7700 Loss 0.9062482118606567\n",
            "Epoch 1 Batch 7800 Loss 1.3591917753219604\n",
            "Epoch 1 Batch 7900 Loss 0.7403473854064941\n",
            "Epoch 1 Batch 8000 Loss 1.1125458478927612\n",
            "Epoch 1 Batch 8100 Loss 0.9561553001403809\n",
            "Epoch 1 Batch 8200 Loss 1.4028351306915283\n",
            "Epoch 1 Batch 8300 Loss 1.5586410760879517\n",
            "Epoch 1 Batch 8400 Loss 0.8408747315406799\n",
            "Epoch 1 Batch 8500 Loss 1.3083245754241943\n",
            "Epoch 1 Batch 8600 Loss 0.9745032787322998\n",
            "Epoch 1 Batch 8700 Loss 1.5854618549346924\n",
            "Epoch 1 Batch 8800 Loss 1.9381552934646606\n",
            "Epoch 1 Batch 8900 Loss 1.2750964164733887\n",
            "Epoch 1 Batch 9000 Loss 1.5898367166519165\n",
            "Epoch 1 Batch 9100 Loss 0.763740599155426\n",
            "Epoch 1 Batch 9200 Loss 1.567655324935913\n",
            "Epoch 1 Batch 9300 Loss 0.9022234082221985\n",
            "Epoch 1 Batch 9400 Loss 0.4298349618911743\n",
            "Epoch 1 Batch 9500 Loss 3.422287702560425\n",
            "Epoch 1 Batch 9600 Loss 0.9930800199508667\n",
            "Epoch 1 Batch 9700 Loss 1.0175305604934692\n",
            "Epoch 1 Batch 9800 Loss 1.2832014560699463\n",
            "Epoch 1 Batch 9900 Loss 0.7967233657836914\n",
            "Epoch 1 Batch 10000 Loss 0.45826542377471924\n",
            "Epoch 1 Batch 10100 Loss 0.9307882785797119\n",
            "Epoch 1 Batch 10200 Loss 0.943549394607544\n",
            "Epoch 1 Batch 10300 Loss 0.9768151044845581\n",
            "Epoch 1 Batch 10400 Loss 1.1595925092697144\n",
            "Epoch 1 Batch 10500 Loss 1.384363055229187\n",
            "Epoch 1 Batch 10600 Loss 1.0606815814971924\n",
            "Epoch 1 Batch 10700 Loss 0.48654472827911377\n",
            "Epoch 1 Batch 10800 Loss 0.7050817608833313\n",
            "Epoch 1 Batch 10900 Loss 1.0370354652404785\n",
            "Epoch 1 Batch 11000 Loss 0.6857638359069824\n",
            "Epoch 1 Batch 11100 Loss 1.1450186967849731\n",
            "Epoch 1 Batch 11200 Loss 0.49639463424682617\n",
            "Epoch 1 Batch 11300 Loss 1.1387145519256592\n",
            "Epoch 1 Batch 11400 Loss 0.9136156439781189\n",
            "Epoch 1 Batch 11500 Loss 1.2113088369369507\n",
            "Epoch 1 Batch 11600 Loss 1.0551977157592773\n",
            "Epoch 1 Batch 11700 Loss 1.170182466506958\n",
            "Epoch 1 Batch 11800 Loss 0.6299198269844055\n",
            "Epoch 1 Batch 11900 Loss 0.650489330291748\n",
            "Epoch 1 Batch 12000 Loss 2.491626501083374\n",
            "Epoch 1 Batch 12100 Loss 0.5455439686775208\n",
            "Epoch 1 Batch 12200 Loss 0.7727088928222656\n",
            "Epoch 1 Batch 12300 Loss 0.5582942962646484\n",
            "Epoch 1 Batch 12400 Loss 0.8759188652038574\n",
            "Epoch 1 Batch 12500 Loss 0.8448947668075562\n",
            "Epoch 1 Batch 12600 Loss 0.35044407844543457\n",
            "Epoch 1 Batch 12700 Loss 1.0459450483322144\n",
            "Epoch 1 Batch 12800 Loss 1.4582643508911133\n",
            "Epoch 1 Batch 12900 Loss 0.6620767116546631\n",
            "Epoch 1 Batch 13000 Loss 0.7832313776016235\n",
            "Epoch 1 Batch 13100 Loss 1.1298202276229858\n",
            "Epoch 1 Batch 13200 Loss 0.9536218643188477\n",
            "Epoch 1 Batch 13300 Loss 0.48694801330566406\n",
            "Epoch 1 Batch 13400 Loss 0.35445070266723633\n",
            "Epoch 1 Batch 13500 Loss 0.9199401140213013\n",
            "Epoch 1 Batch 13600 Loss 0.3315519690513611\n",
            "Epoch 1 Batch 13700 Loss 0.5702962279319763\n",
            "Epoch 1 Batch 13800 Loss 0.6952486038208008\n",
            "Epoch 1 Batch 13900 Loss 0.19414901733398438\n",
            "Epoch 1 Batch 14000 Loss 0.8200769424438477\n",
            "Epoch 1 Batch 14100 Loss 1.085330605506897\n",
            "Epoch 1 Batch 14200 Loss 0.5232452154159546\n",
            "Epoch 1 Batch 14300 Loss 0.3138468265533447\n",
            "Epoch 1 Batch 14400 Loss 0.6082382202148438\n",
            "Epoch 1 Batch 14500 Loss 0.5779011249542236\n",
            "Epoch 1 Batch 14600 Loss 0.1847515106201172\n",
            "Epoch 1 Batch 14700 Loss 0.26122570037841797\n",
            "Epoch 1 Batch 14800 Loss 0.3046985864639282\n",
            "Epoch 1 Batch 14900 Loss 0.4400055408477783\n",
            "Epoch 1 Batch 15000 Loss 0.3699193596839905\n",
            "Epoch 1 Batch 15100 Loss 0.5073599815368652\n",
            "Epoch 1 Batch 15200 Loss 0.44601941108703613\n",
            "Epoch 1 Batch 15300 Loss 0.3956475257873535\n",
            "Epoch 1 Batch 15400 Loss 0.21386420726776123\n",
            "Epoch 1 Batch 15500 Loss 0.10233461856842041\n",
            "Epoch 1 Batch 15600 Loss 0.6482453942298889\n",
            "Epoch 1 Batch 15700 Loss 0.7736279964447021\n",
            "Epoch 1 Batch 15800 Loss 0.355867862701416\n",
            "Epoch 1 Batch 15900 Loss 0.18050265312194824\n",
            "Epoch 1 Batch 16000 Loss 0.47830402851104736\n",
            "Epoch 1 Batch 16100 Loss 0.5236732959747314\n",
            "Epoch 1 Batch 16200 Loss 0.535984218120575\n",
            "Epoch 1 Batch 16300 Loss 0.8296957015991211\n",
            "Epoch 1 Batch 16400 Loss 0.34040623903274536\n",
            "Epoch 1 Batch 16500 Loss 0.17059087753295898\n",
            "Epoch 1 Batch 16600 Loss 0.3183659315109253\n",
            "Epoch 1 Batch 16700 Loss 0.43422532081604004\n",
            "Epoch 1 Batch 16800 Loss 0.23231244087219238\n",
            "Epoch 1 Batch 16900 Loss 0.272122323513031\n",
            "Epoch 1 Batch 17000 Loss 0.35451540350914\n",
            "Epoch 1 Batch 17100 Loss 0.2604043483734131\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}